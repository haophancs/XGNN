{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EwOHpuAZvL97"
   },
   "source": [
    "Meanwhile, for the real-world dataset MUTAG, since all nodes are labeled, we employ the corresponding\n",
    "one-hot representations as the initial node features. Then we employ three layers of GCNs with output dimensions equal to 32, 48,\n",
    "64 respectively and average all node features. The final classifier\n",
    "contains two fully-connected layers in which the hidden dimension\n",
    "is set to 32. Note that for all GCN layers, we apply the GCN version\n",
    "shown in Equation (1). In addition, we employ Sigmoid as the nonlinear function in GCNs for dataset Is_Acyclic while we use Relu for\n",
    "dataset MUTAG. These models are implemented using Pytorch [27]\n",
    "and trained using Adam optimizer [18]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rP2w7zMwC9Uu",
    "outputId": "06ce6d43-e204-4a4e-d4d5-6798728357ff"
   },
   "outputs": [],
   "source": [
    "# Colab compatitbility\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    " \n",
    "# %cd /content/drive/My\\ Drive/Colab\\ Notebooks/URECA/XGNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gWinGsw_T9BS"
   },
   "source": [
    "# Utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FyJhmxNElNAK"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "\n",
    "\n",
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "\n",
    "def load_Mutagenicity_data(path=\"Mutag/\", dataset=\"Mutag\", split_train=0.7, split_val=0.15):\n",
    "    \"\"\"Load Mutagenicity data \"\"\"\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    nodeidx_features = np.genfromtxt(\"{}{}.node_labels\".format(path, dataset), delimiter=\",\",\n",
    "                                        dtype=np.dtype(int))\n",
    "    features = np.zeros((nodeidx_features.shape[0], max(nodeidx_features) + 1))\n",
    "    features[np.arange(nodeidx_features.shape[0]), nodeidx_features] = 1\n",
    "    features = sp.csr_matrix(features, dtype=np.float32)\n",
    "\n",
    "    labels = np.genfromtxt(\"{}{}.graph_labels\".format(path, dataset),\n",
    "                                        dtype=np.dtype(int))\n",
    "    labels = encode_onehot(labels)\n",
    "\n",
    "    graph_idx = np.genfromtxt(\"{}{}.graph_idx\".format(path, dataset),\n",
    "                                        dtype=np.dtype(int))\n",
    "    graph_idx = np.array(graph_idx, dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(graph_idx)}\n",
    "\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.edges\".format(path, dataset), delimiter=\",\",\n",
    "                                    dtype=np.int32)\n",
    "    edges_label = np.genfromtxt(\"{}{}.link_labels\".format(path, dataset), delimiter=\",\",\n",
    "                                    dtype=np.int32)\n",
    "    \n",
    "    # According to paper, ignore edge labels\n",
    "    # adj = sp.coo_matrix((edges_label, (edges_unordered[:,0]-1, edges_unordered[:,1]-1)))\n",
    "    adj = sp.coo_matrix((np.ones(len(edges_label)), (edges_unordered[:,0]-1, edges_unordered[:,1]-1)))\n",
    "\n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    features = normalize(features)\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    num_total = max(graph_idx)\n",
    "    num_train = int(split_train * num_total)\n",
    "    num_val = int((split_train + split_val) * num_total)\n",
    "\n",
    "    if (num_train == num_val or num_val == num_total):\n",
    "        return\n",
    "\n",
    "    idx_train = range(num_train)\n",
    "    idx_val = range(num_train,num_val)\n",
    "    idx_test = range(num_val, num_total)\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "    return adj, features, labels, idx_map, idx_train, idx_val, idx_test\n",
    "\n",
    "def load_MUTAG_data(path=\"MUTAG/\", dataset=\"MUTAG_\", split_train=0.7, split_val=0.15):\n",
    "    \"\"\"Load MUTAG data \"\"\"\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    nodeidx_features = np.genfromtxt(\"{}{}node_labels.txt\".format(path, dataset), delimiter=\",\",\n",
    "                                        dtype=np.dtype(int))\n",
    "    features = np.zeros((nodeidx_features.shape[0], max(nodeidx_features) + 1))\n",
    "    features[np.arange(nodeidx_features.shape[0]), nodeidx_features] = 1\n",
    "    features = sp.csr_matrix(features, dtype=np.float32)\n",
    "\n",
    "    labels = np.genfromtxt(\"{}{}graph_labels.txt\".format(path, dataset),\n",
    "                                        dtype=np.dtype(int))\n",
    "    labels = encode_onehot(labels)\n",
    "\n",
    "    graph_idx = np.genfromtxt(\"{}{}graph_indicator.txt\".format(path, dataset),\n",
    "                                        dtype=np.dtype(int))\n",
    "    graph_idx = np.array(graph_idx, dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(graph_idx)}\n",
    "\n",
    "    edges_unordered = np.genfromtxt(\"{}{}A.txt\".format(path, dataset), delimiter=\",\",\n",
    "                                    dtype=np.int32)\n",
    "    edges_label = np.genfromtxt(\"{}{}edge_labels.txt\".format(path, dataset), delimiter=\",\",\n",
    "                                    dtype=np.int32)\n",
    "    adj = sp.coo_matrix((edges_label, (edges_unordered[:,0]-1, edges_unordered[:,1]-1)))\n",
    "\n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    features = normalize(features)\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    num_total = max(graph_idx)\n",
    "    num_train = int(split_train * num_total)\n",
    "    num_val = int((split_train + split_val) * num_total)\n",
    "\n",
    "    if (num_train == num_val or num_val == num_total):\n",
    "        return\n",
    "\n",
    "    idx_train = range(num_train)\n",
    "    idx_val = range(num_train,num_val)\n",
    "    idx_test = range(num_val, num_total)\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "    return adj, features, labels, idx_map, idx_train, idx_val, idx_test\n",
    "\n",
    "\n",
    "def load_split_MUTAG_data(path=\"MUTAG/\", dataset=\"MUTAG_\", split_train=0.7, split_val=0.15):\n",
    "    \"\"\"Load MUTAG data \"\"\"\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    labels = np.genfromtxt(\"{}{}graph_labels.txt\".format(path, dataset),\n",
    "                                        dtype=np.dtype(int))\n",
    "    labels = encode_onehot(labels)\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "\n",
    "    graph_idx = np.genfromtxt(\"{}{}graph_indicator.txt\".format(path, dataset),\n",
    "                                        dtype=np.dtype(int))\n",
    "    graph_idx = np.array(graph_idx, dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(graph_idx)}\n",
    "    length = len(idx_map.keys())\n",
    "    num_nodes = [idx_map[n] - idx_map[n-1] if n-1>1 else idx_map[n] for n in range(1, length+1)]\n",
    "    max_num_nodes = max(num_nodes)\n",
    "    features_list = []\n",
    "    adj_list = []\n",
    "    prev = 0\n",
    "    \n",
    "    nodeidx_features = np.genfromtxt(\"{}{}node_labels.txt\".format(path, dataset), delimiter=\",\",\n",
    "                                        dtype=np.dtype(int))\n",
    "    features = np.zeros((nodeidx_features.shape[0], max(nodeidx_features) + 1))\n",
    "    features[np.arange(nodeidx_features.shape[0]), nodeidx_features] = 1\n",
    "\n",
    "    edges_unordered = np.genfromtxt(\"{}{}A.txt\".format(path, dataset), delimiter=\",\",\n",
    "                                    dtype=np.int32)\n",
    "    edges_label = np.genfromtxt(\"{}{}edge_labels.txt\".format(path, dataset), delimiter=\",\",\n",
    "                                    dtype=np.int32)\n",
    "    adj = sp.coo_matrix((edges_label, (edges_unordered[:,0]-1, edges_unordered[:,1]-1)))\n",
    "\n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    features = normalize(features)\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "    adj = adj.todense()\n",
    "\n",
    "    for n in range(1, length+1):\n",
    "        entry = np.zeros((max_num_nodes, max(nodeidx_features) + 1))\n",
    "        entry[:idx_map[n] - prev] = features[prev:idx_map[n]]\n",
    "        entry = torch.FloatTensor(entry)\n",
    "        features_list.append(entry)\n",
    "        \n",
    "        entry = np.zeros((max_num_nodes, max_num_nodes))\n",
    "        entry[:idx_map[n] - prev, :idx_map[n] - prev] = adj[prev:idx_map[n], prev:idx_map[n]]\n",
    "        entry = torch.FloatTensor(entry)\n",
    "        adj_list.append(entry)\n",
    "\n",
    "        prev = idx_map[n]\n",
    "\n",
    "    num_total = max(graph_idx)\n",
    "    num_train = int(split_train * num_total)\n",
    "    num_val = int((split_train + split_val) * num_total)\n",
    "\n",
    "    if (num_train == num_val or num_val == num_total):\n",
    "        return\n",
    "\n",
    "    idx_train = range(num_train)\n",
    "    idx_val = range(num_train,num_val)\n",
    "    idx_test = range(num_val, num_total)\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "    return adj_list, features_list, labels, idx_map, idx_train, idx_val, idx_test\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5plpJwEf2OIn"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QfvnkXBf2dwA"
   },
   "source": [
    "Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "hZZgY1uX2c64"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "\n",
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSGqO7HQ2xM1"
   },
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "w-7MdEYQ2RJ8"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nclass, dropout):\n",
    "        \"\"\" As per paper \"\"\"\n",
    "        \"\"\" 3 layers of GCNs with output dimensions equal to 32, 48, 64 respectively and average all node features \"\"\"\n",
    "        \"\"\" Final classifier with 2 fully connected layers and hidden dimension set to 32 \"\"\"\n",
    "        \"\"\" Activation function - ReLu (Mutag) \"\"\"\n",
    "\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, 32)\n",
    "        self.gc2 = GraphConvolution(32, 48)\n",
    "        self.gc3 = GraphConvolution(48, 64)\n",
    "        self.fc1 = nn.Linear(64, 32)\n",
    "        self.fc2 = nn.Linear(32, nclass)\n",
    "\n",
    "    def forward(self, x, adj, idx_map):\n",
    "\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.relu(self.gc2(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.relu(self.gc3(x, adj))\n",
    "\n",
    "        # prev = 0\n",
    "        # y = []\n",
    "        # for idx in idx_map:\n",
    "        #   y.append(torch.mean(x[prev:idx_map[idx]], 0))\n",
    "        #   prev = idx_map[idx]\n",
    "        # y = torch.stack(y, 0)\n",
    "\n",
    "        y = torch.mean(x, 0)\n",
    "\n",
    "        y = F.relu(self.fc1(y))\n",
    "        y = F.dropout(y, self.dropout, training=self.training)\n",
    "        y = F.softmax(self.fc2(y), dim=0)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NP4drwZjCg2z"
   },
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sq4q5O-tqQW5",
    "outputId": "28d28d8a-0337-4a78-9906-9c4353ebe652"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MUTAG_ dataset...\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# adj, features, labels, idx_map, idx_train, idx_val, idx_test = load_MUTAG_data()\n",
    "adj_list, features_list, labels, idx_map, idx_train, idx_val, idx_test = load_split_MUTAG_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "DXQRRd-mAsAe"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Parameters\n",
    "\n",
    "class Object(object):\n",
    "    pass\n",
    "\n",
    "args = Object()\n",
    "args.epochs = 80\n",
    "args.seed = 200\n",
    "args.cuda = torch.cuda.is_available()\n",
    "args.lr = 0.001\n",
    "args.dropout = 0.1\n",
    "args.weight_decay = 5e-4\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# Model and optimizer\n",
    "\n",
    "# model = GCN(nfeat=features.shape[1],\n",
    "#             nclass=labels.max().item() + 1,\n",
    "#             dropout=args.dropout)\n",
    "\n",
    "model = GCN(nfeat=features_list[0].shape[1],\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=args.dropout)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "# if args.cuda:\n",
    "#     model.cuda()\n",
    "#     features = features.cuda()\n",
    "#     adj = adj.cuda()\n",
    "#     labels = labels.cuda()\n",
    "#     idx_train = idx_train.cuda()\n",
    "#     idx_val = idx_val.cuda()\n",
    "#     idx_test = idx_test.cuda()\n",
    "\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "    features = features_list.cuda()\n",
    "    adj = adj_list.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()\n",
    "\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # # Split\n",
    "    output = None\n",
    "    for i in idx_train:\n",
    "      if output is None:\n",
    "        output = model(features_list[i], adj_list[i], idx_map)\n",
    "      else:\n",
    "        output = torch.vstack((output, model(features_list[i], adj_list[i], idx_map)))\n",
    "    loss_train = F.cross_entropy(output, labels[idx_train])\n",
    "    acc_train = accuracy(output, labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    output = None\n",
    "    for i in idx_val:\n",
    "      if output is None:\n",
    "        output = model(features_list[i], adj_list[i], idx_map)\n",
    "      else:\n",
    "        output = torch.vstack((output, model(features_list[i], adj_list[i], idx_map)))\n",
    "    loss_val = F.cross_entropy(output, labels[idx_val])\n",
    "    acc_val = accuracy(output, labels[idx_val])\n",
    "\n",
    "    # # Not split\n",
    "    # output = model(features, adj, idx_map)\n",
    "    # TODO : Determine LOSS FUNCTION\n",
    "    # loss_train = F.cross_entropy(output[idx_train], labels[idx_train])\n",
    "    # acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    # loss_train.backward()\n",
    "    # optimizer.step()\n",
    "\n",
    "    # loss_val = F.cross_entropy(output[idx_val], labels[idx_val])\n",
    "    # acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "    \n",
    "    return loss_train, acc_train, loss_val, acc_val\n",
    "\n",
    "class EarlyStopping():\n",
    "    def __init__(self, patience = 10, min_loss = 0.5, hit_min_before_stopping = False):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.hit_min_before_stopping = hit_min_before_stopping\n",
    "        if hit_min_before_stopping:\n",
    "            self.min_loss = min_loss\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = loss\n",
    "        elif loss > self.best_loss:\n",
    "            self.counter += 1\n",
    "            if self.counter > self.patience:\n",
    "              if self.hit_min_before_stopping == True and loss > self.min_loss:\n",
    "                print(\"Cannot hit mean loss, will continue\")\n",
    "                self.counter -= self.patience\n",
    "              else:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = loss\n",
    "            counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3EPXCB8uu928",
    "outputId": "9b9cd19f-b232-4d81-b84e-34e0658f977b"
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "t_total = time.time()\n",
    "early_stopping = EarlyStopping(10, hit_min_before_stopping=True)\n",
    "\n",
    "for epoch in range(10000):\n",
    "    loss_train, acc_train, loss_val, acc_val = train(epoch)\n",
    "    print(loss_val)\n",
    "    early_stopping(loss_val)\n",
    "    if early_stopping.early_stop == True:\n",
    "        break;\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zn750VouPDLt",
    "outputId": "724f1be3-f69a-4c4a-a0dc-286a038dade5"
   },
   "outputs": [],
   "source": [
    "output = None\n",
    "for i in idx_test:\n",
    "  if output is None:\n",
    "    output = model(features_list[i], adj_list[i], idx_map)\n",
    "  else:\n",
    "    output = torch.vstack((output, model(features_list[i], adj_list[i], idx_map)))\n",
    "loss_test = F.cross_entropy(output, labels[idx_test])\n",
    "acc_test = accuracy(output, labels[idx_test])\n",
    "print(loss_test)\n",
    "print(acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-CWggHMLD-Lm"
   },
   "outputs": [],
   "source": [
    "PATH = \"/content/ckpt\"\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n0soKTmYNKft"
   },
   "source": [
    "# Graph generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5T6KDrYxJdDU"
   },
   "source": [
    "<h2> Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D342EJmwFdZ5"
   },
   "outputs": [],
   "source": [
    "from torch.nn.modules.module import Module\n",
    "from torch.nn import Linear\n",
    "from torch.nn import ReLU6\n",
    "from torch.nn import Sequential\n",
    "import random\n",
    "\n",
    "import copy\n",
    "\n",
    "MAX_NUM_NODES = 28 # for mutag\n",
    "random.seed(200)\n",
    "\n",
    "# import GCN (later when using python file)\n",
    "\n",
    "class Generator(Module):\n",
    "    def __init__(self, \n",
    "                 C: list,\n",
    "                 c=0,\n",
    "                 hyp1=1, \n",
    "                 hyp2=2, \n",
    "                 start=None,\n",
    "                 nfeat=7,\n",
    "                 dropout=0.1):\n",
    "        \"\"\" \n",
    "        :param C: Candidate set of nodes (list)\n",
    "        :param start: Starting node (defaults to randomised node)\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.nfeat = nfeat\n",
    "        self.dropout = dropout\n",
    "        self.c = c\n",
    "\n",
    "        self.fc = Linear(nfeat, 8)\n",
    "        self.gc1 = GraphConvolution(8, 16)\n",
    "        self.gc2 = GraphConvolution(16, 24)\n",
    "        self.gc3 = GraphConvolution(24, 32)\n",
    "\n",
    "        # MLP1\n",
    "        # 2 FC layers with hidden dimension 16\n",
    "        self.mlp1 = Sequential(Linear(32, 16),\n",
    "                               Linear(16, 1))\n",
    "\n",
    "        # MLP2\n",
    "        # 2 FC layers with hidden dimension 24\n",
    "        self.mlp2 = Sequential(Linear(64, 24),\n",
    "                               Linear(24, 1))\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.hyp1 = hyp1\n",
    "        self.hyp2 = hyp2\n",
    "        self.candidate_set = C\n",
    "        \n",
    "        # Default starting node (if any)\n",
    "        if start is not None:\n",
    "          self.start = start\n",
    "          self.random_start = False\n",
    "        else:\n",
    "          self.start = random.choice(np.arange(0, len(self.candidate_set)))\n",
    "          self.random_start = True\n",
    "\n",
    "        # Load GCN for calculating reward\n",
    "        self.model = GCN(nfeat=features_list[0].shape[1],\n",
    "                         nclass=labels.max().item() + 1,\n",
    "                         dropout=args.dropout)\n",
    "        \n",
    "        self.model.load_state_dict(torch.load(PATH))\n",
    "        for param in self.model.parameters():\n",
    "          param.requires_grad = False\n",
    "\n",
    "        self.reset_graph()\n",
    "        \n",
    "    def reset_graph(self):\n",
    "        \"\"\"\n",
    "        Reset g.G to default graph with only start node\n",
    "        \"\"\"\n",
    "        if self.random_start == True:\n",
    "            self.start = random.choice(np.arange(0, len(self.candidate_set)))\n",
    "\n",
    "        mask_start = torch.BoolTensor([False if i == 0 else True for i in range(MAX_NUM_NODES + len(self.candidate_set))])\n",
    "        \n",
    "        adj = torch.zeros((MAX_NUM_NODES + len(self.candidate_set), MAX_NUM_NODES + len(self.candidate_set)), dtype=torch.float32)\n",
    "\n",
    "        feat = torch.zeros((MAX_NUM_NODES + len(self.candidate_set), len(self.candidate_set)), dtype=torch.float32)\n",
    "        feat[0, self.start] = 1\n",
    "        feat[np.arange(-len(self.candidate_set), 0), np.arange(0, len(self.candidate_set))] = 1\n",
    "\n",
    "        degrees = torch.zeros(MAX_NUM_NODES)\n",
    "\n",
    "        self.G = {'adj': adj, 'feat': feat, 'degrees': degrees, 'num_nodes': 1, 'mask_start': mask_start}\n",
    "\n",
    "    def calculate_loss(self, Rt, p_start, a_start, p_end, a_end, G_t_1):\n",
    "        \"\"\"\n",
    "        Calculated from cross entropy loss (Lce) and reward function (Rt)\n",
    "        where loss = -Rt*(Lce_start + Lce_end)\n",
    "        \"\"\"\n",
    "\n",
    "        Lce_start = F.cross_entropy(torch.reshape(p_start, (1, 35)), a_start.unsqueeze(0))\n",
    "        Lce_end = F.cross_entropy(torch.reshape(p_end, (1, 35)), a_end.unsqueeze(0))\n",
    "\n",
    "        return -Rt*(Lce_start + Lce_end)\n",
    "\n",
    "    def calculate_reward(self, G_t_1):\n",
    "        \"\"\"\n",
    "        Rtr     Calculated from graph rules to encourage generated graphs to be valid\n",
    "                1. Only one edge to be added between any two nodes\n",
    "                2. Generated graph cannot contain more nodes than predefined maximum node number\n",
    "                3. (For chemical) Degree cannot exceed valency\n",
    "                If generated graph violates graph rule, Rtr = -1\n",
    "\n",
    "        Rtf     Feedback from trained model\n",
    "        \"\"\"\n",
    "\n",
    "        rtr = self.check_graph_rules(G_t_1)\n",
    "\n",
    "        rtf = self.calculate_reward_feedback(G_t_1)\n",
    "        rtf_sum = 0\n",
    "        for m in range(rollout):\n",
    "            p_start, a_start, p_end, a_end, G_t_1 = self.forward(G_t_1)\n",
    "            rtf_sum += self.calculate_reward_feedback(G_t_1)\n",
    "        rtf = rtf + rtf_sum * self.hyp1 / rollout\n",
    "\n",
    "        return rtf + self.hyp2 * rtr\n",
    "\n",
    "    def calculate_reward_feedback(self, G_t_1):\n",
    "        \"\"\"\n",
    "        p(f(G_t_1) = c) - 1/l\n",
    "        where l denotes number of possible classes for f\n",
    "        \"\"\"\n",
    "        f = self.model(G_t_1['feat'], G_t_1['adj'], None)\n",
    "        return f[self.c] - 1/len(f)\n",
    "\n",
    "    def check_graph_rules(self, G_t_1):\n",
    "        \"\"\"\n",
    "        For mutag, node degrees cannot exceed valency\n",
    "        \"\"\"\n",
    "        idx = 0\n",
    "\n",
    "        for d in G_t_1['degrees']:\n",
    "          if d is not 0:\n",
    "            node_id = torch.argmax(G_t_1['feat'][idx]) # Eg. [0, 1, 0, 0] -> 1\n",
    "            node = self.candidate_set[node_id]  # Eg ['C.4', 'F.2', 'Br.7'][1] = 'F.2'\n",
    "            max_valency = int(node.split('.')[1]) # Eg. C.4 -> ['C', '4'] -> 4\n",
    "\n",
    "            # If any node degree exceeds its valency, return -1\n",
    "            if max_valency < d:\n",
    "                return -1\n",
    "\n",
    "        return 0\n",
    "        \n",
    "    def forward(self, G_in):\n",
    "        G = copy.deepcopy(G_in)\n",
    "\n",
    "        x = G['feat'].detach().clone()\n",
    "        adj = G['adj'].detach().clone()\n",
    "\n",
    "        x = F.relu6(self.fc(x))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.relu6(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.relu6(self.gc2(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.relu6(self.gc3(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "\n",
    "        p_start = self.mlp1(x)\n",
    "        p_start = p_start.masked_fill(G['mask_start'].unsqueeze(1), 0)\n",
    "        p_start = F.softmax(p_start, dim=0)\n",
    "        a_start_idx = torch.argmax(p_start.masked_fill(G['mask_start'].unsqueeze(1), -1))\n",
    "        \n",
    "        # broadcast\n",
    "        x1, x2 = torch.broadcast_tensors(x, x[a_start_idx])\n",
    "        x = torch.cat((x1, x2), 1) # cat increases dim from 32 to 64\n",
    "\n",
    "        mask_end = torch.BoolTensor([True for i in range(MAX_NUM_NODES + len(self.candidate_set))])\n",
    "        mask_end[MAX_NUM_NODES:] = False\n",
    "        mask_end[:G['num_nodes']] = False\n",
    "        mask_end[a_start_idx] = True\n",
    "\n",
    "        p_end = self.mlp2(x)\n",
    "        p_end = p_end.masked_fill(mask_end.unsqueeze(1), 0)\n",
    "        p_end = F.softmax(p_end, dim=0)\n",
    "        a_end_idx = torch.argmax(p_end.masked_fill(mask_end.unsqueeze(1), -1))\n",
    "\n",
    "        # Return new G\n",
    "        # If a_end_idx is not masked, node exists in graph, no new node added\n",
    "        if G['mask_start'][a_end_idx] == False:\n",
    "            G['adj'][a_end_idx][a_start_idx] += 1\n",
    "            G['adj'][a_start_idx][a_end_idx] += 1\n",
    "            \n",
    "            # Update degrees\n",
    "            G['degrees'][a_start_idx] += 1\n",
    "            G['degrees'][G['num_nodes']] += 1\n",
    "        else:\n",
    "            # Add node\n",
    "            G['feat'][G['num_nodes']] = G['feat'][a_end_idx]\n",
    "            # Add edge\n",
    "            G['adj'][G['num_nodes']][a_start_idx] += 1\n",
    "            G['adj'][a_start_idx][G['num_nodes']] += 1\n",
    "            # Update degrees\n",
    "            G['degrees'][a_start_idx] += 1\n",
    "            G['degrees'][G['num_nodes']] += 1\n",
    "\n",
    "            # Update start mask\n",
    "            G_mask_start_copy = G['mask_start'].detach().clone()\n",
    "            G_mask_start_copy[G['num_nodes']] = False\n",
    "            G['mask_start'] = G_mask_start_copy\n",
    "            \n",
    "            G['num_nodes'] += 1\n",
    "\n",
    "        return p_start, a_start_idx, p_end, a_end_idx, G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "phoYQYmlSw75"
   },
   "outputs": [],
   "source": [
    "rollout = 10\n",
    "max_gen_step = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LMDgOdT4pmj9"
   },
   "outputs": [],
   "source": [
    "args.lr = 0.01\n",
    "args.b1 = 0.9\n",
    "args.b2 = 0.99\n",
    "args.hyp1 = 1\n",
    "args.hyp2 = 2\n",
    "\n",
    "candidate_set = ['C.4', 'N.5', 'O.2', 'F.1', 'I.7', 'Cl.7', 'Br.5']\n",
    "g = Generator(candidate_set, c=0, start=0)\n",
    "optimizer = optim.Adam(g.parameters(), lr=args.lr, betas=(args.b1, args.b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R66u9R5WFzGY"
   },
   "outputs": [],
   "source": [
    "def train_generator(c=0, \n",
    "                    initial_node=None,\n",
    "                    max_nodes=5):\n",
    "  g.c = c\n",
    "\n",
    "  for i in range(max_gen_step):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    G = copy.deepcopy(g.G)\n",
    "    p_start, a_start, p_end, a_end, G = g.forward(G)\n",
    "\n",
    "    Rt = g.calculate_reward(G)\n",
    "    loss = g.calculate_loss(Rt, p_start, a_start, p_end, a_end, G)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if G['num_nodes'] > max_nodes:\n",
    "      g.reset_graph()\n",
    "    elif Rt > 0:\n",
    "      g.G = G\n",
    "\n",
    "def generate_graph(c=0, max_nodes=5):\n",
    "  g.c = c\n",
    "  g.reset_graph()\n",
    "\n",
    "  for i in range(max_gen_step):\n",
    "    G = copy.deepcopy(g.G)\n",
    "    p_start, a_start, p_end, a_end, G = g.forward(G)\n",
    "    Rt = g.calculate_reward(G)\n",
    "\n",
    "    if G['num_nodes'] > max_nodes:\n",
    "      return g.G\n",
    "    elif Rt > 0:\n",
    "      g.G = G\n",
    "    \n",
    "  return g.G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rURRIXGoK0Hb"
   },
   "source": [
    "Visualizing graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bfVsy8X-Ab1a"
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_graph(G):\n",
    "  G_nx = nx.from_numpy_matrix(np.asmatrix(G['adj'][:G['num_nodes'], :G['num_nodes']].numpy()))\n",
    "  # nx.draw_networkx(G_nx)\n",
    "\n",
    "  layout=nx.spring_layout(G_nx)\n",
    "  nx.draw(G_nx, layout)\n",
    "\n",
    "  coloring=torch.argmax(G['feat'],1)\n",
    "  colors=['b','g','r','c','m','y','k']\n",
    "\n",
    "  for i in range(7):\n",
    "    nx.draw_networkx_nodes(G_nx,pos=layout,nodelist=[x for x in G_nx.nodes() if coloring[x]==i],node_color=colors[i])\n",
    "    nx.draw_networkx_labels(G_nx,pos=layout,labels={x:candidate_set[i].split('.')[0] for x in G_nx.nodes() if coloring[x]==i})\n",
    "  nx.draw_networkx_edges(G_nx,pos=layout,width=list(nx.get_edge_attributes(G_nx,'weight').values()))\n",
    "  nx.draw_networkx_edge_labels(G_nx,pos=layout,edge_labels=nx.get_edge_attributes(G_nx, \"weight\"))\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "goIcA-sgKqzi"
   },
   "source": [
    "Train graph with different max_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6vnCRE6kOjpQ",
    "outputId": "6671153b-eb57-4ba9-a980-28b93e4a73e1"
   },
   "outputs": [],
   "source": [
    "for i in range(1, 10):\n",
    "  g.reset_graph()\n",
    "  train_generator(c=1, initial_node=0, max_nodes=i)\n",
    "  to_display = generate_graph(c=1,max_nodes=i)\n",
    "  display_graph(to_display)\n",
    "  print(g.model(to_display['feat'],to_display['adj'],None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IgLhUBmYJYnf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "XGNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
